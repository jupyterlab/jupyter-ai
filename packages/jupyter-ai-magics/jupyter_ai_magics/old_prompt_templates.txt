# for magics

model_kwargs["prompt_templates"] = {
    "code": PromptTemplate.from_template(
        "{prompt}\n\nProduce output as source code only, "
        "with no text or explanation before or after it."
    ),
    "html": PromptTemplate.from_template(
        "{prompt}\n\nProduce output in HTML format only, "
        "with no markup before or afterward."
    ),
    "image": PromptTemplate.from_template(
        "{prompt}\n\nProduce output as an image only, "
        "with no text before or after it."
    ),
    "markdown": PromptTemplate.from_template(
        "{prompt}\n\nProduce output in markdown format only."
    ),
    "md": PromptTemplate.from_template(
        "{prompt}\n\nProduce output in markdown format only."
    ),
    "math": PromptTemplate.from_template(
        "{prompt}\n\nProduce output in LaTeX format only, "
        "with $$ at the beginning and end."
    ),
    "json": PromptTemplate.from_template(
        "{prompt}\n\nProduce output in JSON format only, "
        "with nothing before or after it."
    ),
    "text": PromptTemplate.from_template("{prompt}"),  # No customization
}



# for completions

COMPLETION_SYSTEM_PROMPT = """
You are an application built to provide helpful code completion suggestions.
You should only produce code. Keep comments to minimum, use the
programming language comment syntax. Produce clean code.
The code is written in JupyterLab, a data analysis and code development
environment which can execute code extended with additional syntax for
interactive features, such as magics.
""".strip()

# only add the suffix bit if present to save input tokens/computation time
COMPLETION_DEFAULT_TEMPLATE = """
The document is called `{{filename}}` and written in {{language}}.
{% if suffix %}
The code after the completion request is:

```
{{suffix}}
```
{% endif %}

Complete the following code:

```
{{prefix}}"""




# old completion handling code

async def generate_inline_completions(
    self, request: InlineCompletionRequest
) -> InlineCompletionReply:
    chain = self._create_completion_chain()
    model_arguments = completion.template_inputs_from_request(request)
    suggestion = await chain.ainvoke(input=model_arguments)
    suggestion = completion.post_process_suggestion(suggestion, request)
    return InlineCompletionReply(
        list=InlineCompletionList(items=[{"insertText": suggestion}]),
        reply_to=request.number,
    )

async def stream_inline_completions(
    self, request: InlineCompletionRequest
) -> AsyncIterator[InlineCompletionStreamChunk]:
    chain = self._create_completion_chain()
    token = completion.token_from_request(request, 0)
    model_arguments = completion.template_inputs_from_request(request)
    suggestion = processed_suggestion = ""

    # send an incomplete `InlineCompletionReply`, indicating to the
    # client that LLM output is about to streamed across this connection.
    yield InlineCompletionReply(
        list=InlineCompletionList(
            items=[
                {
                    # insert text starts empty as we do not pre-generate any part
                    "insertText": "",
                    "isIncomplete": True,
                    "token": token,
                }
            ]
        ),
        reply_to=request.number,
    )

    async for fragment in chain.astream(input=model_arguments):
        suggestion += fragment
        processed_suggestion = completion.post_process_suggestion(
            suggestion, request
        )
        yield InlineCompletionStreamChunk(
            type="stream",
            response={"insertText": processed_suggestion, "token": token},
            reply_to=request.number,
            done=False,
        )

    # finally, send a message confirming that we are done
    yield InlineCompletionStreamChunk(
        type="stream",
        response={"insertText": processed_suggestion, "token": token},
        reply_to=request.number,
        done=True,
    )
