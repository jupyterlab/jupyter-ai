import base64
import io
import json
from collections.abc import Coroutine
from typing import (
    Any,
    Optional,
)

from langchain.prompts import (
    PromptTemplate,
)
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_community.llms import AI21, GPT4All, HuggingFaceEndpoint, Together

# for backward compatibility the import group below includes
# imports of objects which used to be defined here but were
# only used by BaseProvider that has now moved to `.base_provider`.
from .base_provider import (
    CHAT_DEFAULT_TEMPLATE,
    CHAT_SYSTEM_PROMPT,
    COMPLETION_DEFAULT_TEMPLATE,
    COMPLETION_SYSTEM_PROMPT,
    HUMAN_MESSAGE_TEMPLATE,
    AuthStrategy,
    AwsAuthStrategy,
    BaseProvider,
    EnvAuthStrategy,
    Field,
    IntegerField,
    MultiEnvAuthStrategy,
    MultilineTextField,
    TextField,
)


class AI21Provider(BaseProvider, AI21):
    id = "ai21"
    name = "AI21"
    models = [
        "j1-large",
        "j1-grande",
        "j1-jumbo",
        "j1-grande-instruct",
        "j2-large",
        "j2-grande",
        "j2-jumbo",
        "j2-grande-instruct",
        "j2-jumbo-instruct",
    ]
    model_id_key = "model"
    pypi_package_deps = ["ai21"]
    auth_strategy = EnvAuthStrategy(name="AI21_API_KEY")

    async def _acall(self, *args, **kwargs) -> Coroutine[Any, Any, str]:
        return await self._call_in_executor(*args, **kwargs)

    @classmethod
    def is_api_key_exc(cls, e: Exception):
        """
        Determine if the exception is an AI21 API key error.
        """
        if isinstance(e, ValueError):
            return "status code 401" in str(e)
        return False


class GPT4AllProvider(BaseProvider, GPT4All):
    def __init__(self, **kwargs):
        model = kwargs.get("model_id")
        if model == "ggml-gpt4all-l13b-snoozy":
            kwargs["backend"] = "llama"
        else:
            kwargs["backend"] = "gptj"

        kwargs["allow_download"] = False
        n_threads = kwargs.get("n_threads", None)
        if n_threads is not None:
            kwargs["n_threads"] = max(int(n_threads), 1)
        super().__init__(**kwargs)

    id = "gpt4all"
    name = "GPT4All"
    models = [
        "ggml-gpt4all-j-v1.2-jazzy",
        "ggml-gpt4all-j-v1.3-groovy",
        # this one needs llama backend and has licence restriction
        "ggml-gpt4all-l13b-snoozy",
        "mistral-7b-openorca.Q4_0",
        "mistral-7b-instruct-v0.1.Q4_0",
        "gpt4all-falcon-q4_0",
        "wizardlm-13b-v1.2.Q4_0",
        "nous-hermes-llama2-13b.Q4_0",
        "gpt4all-13b-snoozy-q4_0",
        "mpt-7b-chat-merges-q4_0",
        "orca-mini-3b-gguf2-q4_0",
        "starcoder-q4_0",
        "rift-coder-v0-7b-q4_0",
        "em_german_mistral_v01.Q4_0",
    ]
    model_id_key = "model"
    pypi_package_deps = ["gpt4all"]
    auth_strategy = None
    fields = [IntegerField(key="n_threads", label="CPU thread count (optional)")]

    async def _acall(self, *args, **kwargs) -> Coroutine[Any, Any, str]:
        return await self._call_in_executor(*args, **kwargs)

    @property
    def allows_concurrency(self):
        # At present, GPT4All providers fail with concurrent messages. See #481.
        return False


# References for using HuggingFaceEndpoint and InferenceClient:
# https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient
# https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py
class HfHubProvider(BaseProvider, HuggingFaceEndpoint):
    id = "huggingface_hub"
    name = "Hugging Face Hub"
    models = ["*"]
    model_id_key = "repo_id"
    help = (
        "See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. "
        "Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`."
    )
    # ipywidgets needed to suppress tqdm warning
    # https://stackoverflow.com/questions/67998191
    # tqdm is a dependency of huggingface_hub
    pypi_package_deps = ["huggingface_hub", "ipywidgets"]
    auth_strategy = EnvAuthStrategy(name="HUGGINGFACEHUB_API_TOKEN")
    registry = True

    # Handle text and image outputs
    def _call(
        self, prompt: str, stop: Optional[list[str]] = None, **kwargs: Any
    ) -> str:
        """Call out to Hugging Face Hub's inference endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string or image generated by the model.

        Example:
            .. code-block:: python

                response = hf("Tell me a joke.")
        """
        invocation_params = self._invocation_params(stop, **kwargs)
        invocation_params["stop"] = invocation_params[
            "stop_sequences"
        ]  # porting 'stop_sequences' into the 'stop' argument
        response = self.client.post(
            json={"inputs": prompt, "parameters": invocation_params},
            stream=False,
            task=self.task,
        )

        try:
            if "generated_text" in str(response):
                # text2 text or text-generation task
                response_text = json.loads(response.decode())[0]["generated_text"]
                # Maybe the generation has stopped at one of the stop sequences:
                # then we remove this stop sequence from the end of the generated text
                for stop_seq in invocation_params["stop_sequences"]:
                    if response_text[-len(stop_seq) :] == stop_seq:
                        response_text = response_text[: -len(stop_seq)]
                return response_text
            else:
                # text-to-image task
                # https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_to_image.example
                # Custom code for responding to image generation responses
                image = self.client.text_to_image(prompt)
                imageFormat = image.format  # Presume it's a PIL ImageFile
                mimeType = ""
                if imageFormat == "JPEG":
                    mimeType = "image/jpeg"
                elif imageFormat == "PNG":
                    mimeType = "image/png"
                elif imageFormat == "GIF":
                    mimeType = "image/gif"
                else:
                    raise ValueError(f"Unrecognized image format {imageFormat}")
                buffer = io.BytesIO()
                image.save(buffer, format=imageFormat)
                # # Encode image data to Base64 bytes, then decode bytes to str
                return (
                    mimeType + ";base64," + base64.b64encode(buffer.getvalue()).decode()
                )
        except:
            raise ValueError(
                "Task not supported, only text-generation and text-to-image tasks are valid."
            )

    async def _acall(self, *args, **kwargs) -> Coroutine[Any, Any, str]:
        return await self._call_in_executor(*args, **kwargs)


class TogetherAIProvider(BaseProvider, Together):
    id = "togetherai"
    name = "Together AI"
    model_id_key = "model"
    models = [
        "Austism/chronos-hermes-13b",
        "DiscoResearch/DiscoLM-mixtral-8x7b-v2",
        "EleutherAI/llemma_7b",
        "Gryphe/MythoMax-L2-13b",
        "Meta-Llama/Llama-Guard-7b",
        "Nexusflow/NexusRaven-V2-13B",
        "NousResearch/Nous-Capybara-7B-V1p9",
        "NousResearch/Nous-Hermes-2-Yi-34B",
        "NousResearch/Nous-Hermes-Llama2-13b",
        "NousResearch/Nous-Hermes-Llama2-70b",
    ]
    pypi_package_deps = ["together"]
    auth_strategy = EnvAuthStrategy(name="TOGETHER_API_KEY")

    def __init__(self, **kwargs):
        model = kwargs.get("model_id")

        if model not in self.models:
            kwargs["responses"] = [
                "Model not supported! Please check model list with %ai list"
            ]

        super().__init__(**kwargs)

    def get_prompt_template(self, format) -> PromptTemplate:
        if format == "code":
            return PromptTemplate.from_template(
                "{prompt}\n\nProduce output as source code only, "
                "with no text or explanation before or after it."
            )
        return super().get_prompt_template(format)


# Baidu QianfanChat provider. temporarily living as a separate class until
class QianfanProvider(BaseProvider, QianfanChatEndpoint):
    id = "qianfan"
    name = "ERNIE-Bot"
    models = ["ERNIE-Bot", "ERNIE-Bot-4"]
    model_id_key = "model_name"
    pypi_package_deps = ["qianfan"]
    auth_strategy = MultiEnvAuthStrategy(names=["QIANFAN_AK", "QIANFAN_SK"])
